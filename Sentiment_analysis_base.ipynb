{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение тональности текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используется выборка с отзывами на фильмы [sentence polarity dataset](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz)\n",
    "В ней содержатся предложения с отрицательными и положительными отзывами (по 5331)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import collections, itertools, functools\n",
    "import nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews, stopwords\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "POLARITY_DATA_DIR = os.path.join('polarityData', 'rt-polaritydata')\n",
    "RT_POLARITY_POS_FILE = os.path.join(POLARITY_DATA_DIR, 'rt-polarity-pos.txt')\n",
    "RT_POLARITY_NEG_FILE = os.path.join(POLARITY_DATA_DIR, 'rt-polarity-neg.txt')\n",
    "\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначальная модель: наивный Байес. Каждому предложению сопоставляется бинарный bag-of-words вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def use_all(words):\n",
    "    return dict([(word, True) for word in words])\n",
    " \n",
    "def getWords(file_path, type_of_feature=None, feature_select=None, apply_features=True):\n",
    "    features = []\n",
    "    with open(file_path, 'r') as sentences:\n",
    "        for sentence in sentences:\n",
    "            words = re.findall(r\"[\\w']+|[.,!?;]\", sentence.rstrip())\n",
    "            if apply_features:\n",
    "                words = [feature_select(words), type_of_feature]\n",
    "            features.append(words)\n",
    "    return features\n",
    "\n",
    "def evaluate(feature_selector, show_features=False):\n",
    "    pos_features = getWords(RT_POLARITY_POS_FILE, 'pos', feature_select=feature_selector)\n",
    "    neg_features = getWords(RT_POLARITY_NEG_FILE, 'neg', feature_select=feature_selector)\n",
    "    train_fraction = int(len(pos_features)*3/4)\n",
    "    \n",
    "    train_features = neg_features[:train_fraction] + pos_features[:train_fraction]\n",
    "    test_features = neg_features[train_fraction:] + pos_features[train_fraction:]\n",
    "    clf = NaiveBayesClassifier.train(train_features)\n",
    "    actual_labels = collections.defaultdict(set)\n",
    "    pred_lables = collections.defaultdict(set)\n",
    "    for num, (features, label) in enumerate(test_features):\n",
    "            actual_labels[label].add(num)\n",
    "            observed = clf.classify(features)\n",
    "            pred_lables[observed].add(num)\n",
    " \n",
    "    print 'accuracy:', nltk.classify.util.accuracy(clf, test_features)\n",
    "    print 'precision:', nltk.metrics.precision(actual_labels['pos'], pred_lables['pos'])\n",
    "    print 'recall:', nltk.metrics.recall(actual_labels['pos'], pred_lables['pos'])\n",
    "    if show_features:\n",
    "        clf.show_most_informative_features()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.773068267067\n",
      "precision: 0.787066246057\n",
      "recall: 0.748687171793\n"
     ]
    }
   ],
   "source": [
    "evaluate(use_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем повысить качество модели. Вначале уберем стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.764066016504\n",
      "precision: 0.765060240964\n",
      "recall: 0.762190547637\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    " \n",
    "def stopword_filtered_word_feats(words):\n",
    "    return dict([(word, True) for word in words if word not in stopset])\n",
    " \n",
    "evaluate(stopword_filtered_word_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество только ухудшилось, следовательно стоп-слова помогают в определении тональности. Попробуем добавить в признаки самые значимые биграммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.781695423856\n",
      "precision: 0.791311093871\n",
      "recall: 0.765191297824\n"
     ]
    }
   ],
   "source": [
    "def bigram_word_feats(words, score_fn=BigramAssocMeasures.chi_sq, n=50):\n",
    "    all_bigrams = BigramCollocationFinder.from_words(words)\n",
    "    bigrams = all_bigrams.nbest(score_fn, n)\n",
    "    return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    "\n",
    "evaluate(bigram_word_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество немного выросло, теперь попробуем понизить размерность выборки, используя хи-квадрат метрику."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.846586646662\n",
      "precision: 0.867834394904\n",
      "recall: 0.817704426107\n"
     ]
    }
   ],
   "source": [
    "def get_best_words(num=10000):\n",
    "    word_freq = FreqDist()\n",
    "    label_freq = ConditionalFreqDist()\n",
    "\n",
    "    for word in itertools.chain(*getWords(RT_POLARITY_POS_FILE, 'pos', apply_features=False)):\n",
    "        word_freq[word.lower()] += 1\n",
    "        label_freq['pos'][word.lower()] += 1\n",
    "\n",
    "    for word in  itertools.chain(*getWords(RT_POLARITY_NEG_FILE, 'neg', apply_features=False)):\n",
    "        word_freq[word.lower()] += 1\n",
    "        label_freq['neg'][word.lower()] += 1\n",
    "\n",
    "    pos_word_count = label_freq['pos'].N()\n",
    "    neg_word_count = label_freq['neg'].N()\n",
    "    total_word_count = pos_word_count + neg_word_count\n",
    "\n",
    "    word_scores = {}\n",
    "\n",
    "    for word, freq in word_freq.iteritems():\n",
    "        pos_score = BigramAssocMeasures.chi_sq(label_freq['pos'][word],\n",
    "            (freq, pos_word_count), total_word_count)\n",
    "        neg_score = BigramAssocMeasures.chi_sq(label_freq['neg'][word],\n",
    "            (freq, neg_word_count), total_word_count)\n",
    "        word_scores[word] = pos_score + neg_score\n",
    "\n",
    "    best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:num]\n",
    "    bestwords = set([w for w, s in best])\n",
    "    return bestwords\n",
    " \n",
    "def best_word_feats(words, bestwords):\n",
    "    return dict([(word, True) for word in words if word in bestwords])\n",
    " \n",
    "evaluate(functools.partial(best_word_feats, bestwords=get_best_words()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь качество улучшилось более значительно, посмотрим на ключевые слова, позволяющие определить тональность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.846586646662\n",
      "precision: 0.867834394904\n",
      "recall: 0.817704426107\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     17.0 : 1.0\n",
      "                   quiet = True              pos : neg    =     15.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "                portrait = True              pos : neg    =     12.4 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "                   flaws = True              pos : neg    =     12.3 : 1.0\n",
      "              refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "                 triumph = True              pos : neg    =     11.7 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(functools.partial(best_word_feats, bestwords=get_best_words()), show_features=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим несколько линейных моделей, используя бинарный bag-of-words с отбором самых значимых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import numpy as np\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "def get_feature_matrix(filenames):\n",
    "    best_words = get_best_words()\n",
    "    pos_features = getWords(filenames[0], apply_features=False)\n",
    "    neg_features = getWords(filenames[1], apply_features=False)\n",
    "    features = [dict((x, True) for x in words if x in best_words) for words in\n",
    "                itertools.chain(pos_features, neg_features)]\n",
    "    dv = DictVectorizer()\n",
    "    mat = dv.fit_transform(features)\n",
    "    return mat[:len(pos_features)], mat[len(pos_features):]\n",
    "\n",
    "pos_X, neg_X = get_feature_matrix([RT_POLARITY_POS_FILE, RT_POLARITY_NEG_FILE])\n",
    "\n",
    "pos_Y = np.ones(pos_X.shape[0])\n",
    "neg_Y = np.zeros(neg_X.shape[0])\n",
    "\n",
    "cutoff = int(pos_X.shape[0] * 3/4)\n",
    "train_X = vstack((pos_X[:cutoff], neg_X[:cutoff]))\n",
    "test_X = vstack((pos_X[cutoff:], neg_X[cutoff:]))\n",
    "\n",
    "train_Y = np.concatenate((pos_Y[:cutoff], neg_Y[:cutoff]))\n",
    "test_Y = np.concatenate((pos_Y[cutoff:], neg_Y[cutoff:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.001,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=20, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=0, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.81      0.78      0.79      1333\n",
      "        1.0       0.79      0.81      0.80      1333\n",
      "\n",
      "avg / total       0.80      0.79      0.79      2666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import grid_search\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def benchmark(clf_class, params, name):\n",
    "    clf = grid_search.GridSearchCV(clf_class, params)\n",
    "    clf.fit(train_X, train_Y)\n",
    "\n",
    "    print(clf.best_estimator_)\n",
    "    print()\n",
    "    pred = clf.predict(test_X)\n",
    "    print(classification_report(test_Y, pred))\n",
    "\n",
    "parameters = {\n",
    "    'loss': ['hinge', 'log', 'squared_hinge', 'perceptron'],\n",
    "    'penalty': ['l1','l2', 'elasticnet'],\n",
    "    'alpha': [1e-5, 1e-3, 2e-2, 1e-1],\n",
    "    'l1_ratio': [1e-3, 1e-2, 1e-1]\n",
    "}\n",
    "\n",
    "\n",
    "benchmark(SGDClassifier(n_iter=20,  random_state=0), parameters, 'SGD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr',\n",
      "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
      "          verbose=0)\n",
      "()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.82      0.78      0.80      1333\n",
      "        1.0       0.79      0.83      0.81      1333\n",
      "\n",
      "avg / total       0.81      0.81      0.81      2666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "parameters_lr = {\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    'C' : [1e-4, 1e-3, 1e-1, 10, 100, 1e3]\n",
    "}\n",
    "benchmark(LogisticRegression(random_state=0), parameters_lr, 'LogisticRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
      "     verbose=0)\n",
      "()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.82      0.77      0.79      1333\n",
      "        1.0       0.78      0.83      0.80      1333\n",
      "\n",
      "avg / total       0.80      0.80      0.80      2666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "parameters_svm = {\n",
    "    'penalty': [ 'l1', 'l2'],\n",
    "    'loss': ['squared_hinge'],\n",
    "    'C': [1e-8, 1e-6, 1e-4, 1e-2, 1e0, 1e2, 1e4, 1e6]\n",
    "}\n",
    "benchmark(LinearSVC(random_state=0, dual=False), parameters_svm, 'LinearSVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили примерно одно и то же качество для всех моделей, немного уступающее наивному байесовскому классификатору."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
